WEBVTT

1
00:00:06.320 --> 00:00:09.090
Hi, I'm delighted to
have with us here

2
00:00:09.090 --> 00:00:11.730
today my old friend,
professor Fei-Fei Li.

3
00:00:11.730 --> 00:00:14.640
Fei-Fei is a professor
of computer science at

4
00:00:14.640 --> 00:00:18.450
Stanford University and
also co-director of HAI;

5
00:00:18.450 --> 00:00:21.180
the Human-Centered AI Institute.

6
00:00:21.180 --> 00:00:24.735
Previously, she also
was responsible for

7
00:00:24.735 --> 00:00:28.590
AI at Google Cloud as a chief
scientist for the division.

8
00:00:28.590 --> 00:00:30.120
It's great to have you Fei-Fei.

9
00:00:30.120 --> 00:00:33.100
Thank you, Andrew.
Very happy to be here.

10
00:00:33.110 --> 00:00:36.450
How long have we known each
other? I've lost track.

11
00:00:36.450 --> 00:00:38.400
Definitely more than a decade.

12
00:00:38.400 --> 00:00:44.150
I've known your work before
we even met and I came to

13
00:00:44.150 --> 00:00:50.840
Stanford 2009 but we started
talking 2007, so 15 years.

14
00:00:50.840 --> 00:00:54.095
I actually still have
very clear memories of

15
00:00:54.095 --> 00:00:58.260
how stressful it was when
collectively bunch of us;

16
00:00:58.260 --> 00:00:59.970
me, Chris Manning, bunch of us.

17
00:00:59.970 --> 00:01:01.160
We tried to figure
out how to recruit

18
00:01:01.160 --> 00:01:02.615
you to come to Stanford.

19
00:01:02.615 --> 00:01:03.965
It wasn't hard.

20
00:01:03.965 --> 00:01:07.625
I just needed to sort
out my student's life,

21
00:01:07.625 --> 00:01:11.965
but it's hard to resist Stanford

22
00:01:11.965 --> 00:01:14.430
Really great to have you as
a friend and colleague here.

23
00:01:14.430 --> 00:01:16.880
Me too. It's been a long time

24
00:01:16.880 --> 00:01:20.390
and we're very lucky
to be the generation.

25
00:01:20.390 --> 00:01:23.760
Seeing AI is great progress.

26
00:01:23.760 --> 00:01:25.960
There was something
about your background

27
00:01:25.960 --> 00:01:27.695
I always found inspiring,

28
00:01:27.695 --> 00:01:30.635
which is today people
are entering AI from

29
00:01:30.635 --> 00:01:34.580
all walks of life and
sometimes people still wonder,

30
00:01:34.580 --> 00:01:39.460
is AI a right path for me?

31
00:01:39.460 --> 00:01:41.390
So I thought one of the
most interesting parts

32
00:01:41.390 --> 00:01:42.410
of your background was that

33
00:01:42.410 --> 00:01:43.535
you actually started out

34
00:01:43.535 --> 00:01:45.110
not studying computer
science or AI,

35
00:01:45.110 --> 00:01:48.770
but you started out studying
physics and then had

36
00:01:48.770 --> 00:01:50.840
this path to becoming one of

37
00:01:50.840 --> 00:01:53.585
the most globally
recognizable AI scientists.

38
00:01:53.585 --> 00:01:57.450
How did you make that
switch from physics to AI?

39
00:01:57.450 --> 00:01:59.490
Well, that's a great
question, Andrew,

40
00:01:59.490 --> 00:02:02.085
especially both of us
are passionate about

41
00:02:02.085 --> 00:02:08.085
young people's future and they
come into the world of AI.

42
00:02:08.085 --> 00:02:10.760
The truth is, if I could enter

43
00:02:10.760 --> 00:02:14.000
AI back then more than
20 years ago today,

44
00:02:14.000 --> 00:02:17.180
anybody can enter AI
because AI has become

45
00:02:17.180 --> 00:02:23.450
such a prevailing and globally
impactful technology.

46
00:02:23.450 --> 00:02:27.625
But myself, maybe
I was an accident.

47
00:02:27.625 --> 00:02:32.885
I have always been a
physics kid or a STEM kid.

48
00:02:32.885 --> 00:02:34.880
I'm sure you were too,

49
00:02:34.880 --> 00:02:36.800
but physics was my
passion all the

50
00:02:36.800 --> 00:02:39.350
way through middle school,

51
00:02:39.350 --> 00:02:41.210
high school, college and I went

52
00:02:41.210 --> 00:02:43.745
to Princeton and
majored in physics.

53
00:02:43.745 --> 00:02:47.615
One thing physics has
taught me till today

54
00:02:47.615 --> 00:02:51.710
is really the passion for
asking big questions,

55
00:02:51.710 --> 00:02:55.055
the passion for
seeking no stars.

56
00:02:55.055 --> 00:02:56.990
I was really having fun as

57
00:02:56.990 --> 00:02:59.390
a physics student at Princeton.

58
00:02:59.390 --> 00:03:01.520
One thing I did was reading

59
00:03:01.520 --> 00:03:04.910
up stories and just writings of

60
00:03:04.910 --> 00:03:07.730
great physicists of
the 20th century and

61
00:03:07.730 --> 00:03:12.005
just hear about what they
think about the world,

62
00:03:12.005 --> 00:03:14.885
especially people
like Albert Einstein,

63
00:03:14.885 --> 00:03:18.755
Roger Penrose,
Erwin Schrodinger,

64
00:03:18.755 --> 00:03:22.220
and it was really funny to

65
00:03:22.220 --> 00:03:26.150
notice that many of
the writings towards

66
00:03:26.150 --> 00:03:30.125
the later half of the career
of these great physicists

67
00:03:30.125 --> 00:03:32.840
were not about just
the atomic world

68
00:03:32.840 --> 00:03:34.535
or the physical world,

69
00:03:34.535 --> 00:03:36.320
but ponderings about

70
00:03:36.320 --> 00:03:41.420
equally audacious
questions like life,

71
00:03:41.420 --> 00:03:44.585
like intelligence,
like human conditions.

72
00:03:44.585 --> 00:03:47.495
Schrodinger wrote this
book, What is Life?

73
00:03:47.495 --> 00:03:54.160
Roger Penrose wrote this
book, Emperor's New Mind.

74
00:03:54.620 --> 00:03:57.710
That really got me very curious

75
00:03:57.710 --> 00:04:00.575
about the topic of intelligence.

76
00:04:00.575 --> 00:04:03.590
One thing led to another
during college time,

77
00:04:03.590 --> 00:04:07.595
I did intern at the top
of neuroscience labs

78
00:04:07.595 --> 00:04:12.245
and especially vision-related,
and I was like,

79
00:04:12.245 --> 00:04:16.865
wow, this is just as
audacious question to ask as

80
00:04:16.865 --> 00:04:22.020
the beginning of the universe
or what is matter made of?

81
00:04:22.020 --> 00:04:26.120
That got me to switch
from undergraduate degree

82
00:04:26.120 --> 00:04:30.040
in physics to graduate
degree in AI.

83
00:04:30.040 --> 00:04:33.855
Even though I don't know
about you during our time,

84
00:04:33.855 --> 00:04:36.220
AI was a dirty word.

85
00:04:36.220 --> 00:04:37.670
It was AI winter,

86
00:04:37.670 --> 00:04:39.620
so it was more machine
learning and computer

87
00:04:39.620 --> 00:04:42.425
vision, and computational
neuroscience.

88
00:04:42.425 --> 00:04:45.605
Yeah, I know. Honestly, I
think when I was an undergrad,

89
00:04:45.605 --> 00:04:47.345
I was too busy writing code.

90
00:04:47.345 --> 00:04:49.940
I just managed to blithely

91
00:04:49.940 --> 00:04:52.550
ignore the AI winter and
just kept on coding.

92
00:04:52.550 --> 00:04:57.870
Yeah, well, I was too busy
solving PDE equations.

93
00:04:58.240 --> 00:05:02.450
Actually, do you have an
audacious question now?

94
00:05:02.450 --> 00:05:06.230
Yes, my audacious question
is still intelligence.

95
00:05:06.230 --> 00:05:08.945
I think since Alan Turing,

96
00:05:08.945 --> 00:05:12.755
humanity has not fully
understand what is

97
00:05:12.755 --> 00:05:18.360
the fundamental computing
principles behind intelligence.

98
00:05:20.100 --> 00:05:22.960
Today we use the words AI,

99
00:05:22.960 --> 00:05:24.370
we use the word AGI,

100
00:05:24.370 --> 00:05:26.110
but at the end of the day,

101
00:05:26.110 --> 00:05:30.700
I still dream of a set
of simple equations or

102
00:05:30.700 --> 00:05:32.560
simple principles that can

103
00:05:32.560 --> 00:05:35.680
define the process
of intelligence,

104
00:05:35.680 --> 00:05:40.080
whether it's animal intelligence
or machine intelligence.

105
00:05:40.080 --> 00:05:42.570
This is similar to physics.

106
00:05:42.570 --> 00:05:43.830
For example, many people

107
00:05:43.830 --> 00:05:46.605
have drawn the
analogy of flying.

108
00:05:46.605 --> 00:05:50.050
Are we replicating birds

109
00:05:50.050 --> 00:05:52.420
flying or are we
building airplane?

110
00:05:52.420 --> 00:05:54.190
A lot of people ask
the question of

111
00:05:54.190 --> 00:05:57.505
the relationship
between AI and brain.

112
00:05:57.505 --> 00:05:59.140
To me, whether we're

113
00:05:59.140 --> 00:06:02.260
building a bird or

114
00:06:02.260 --> 00:06:04.585
replicating a bird or
building an airplane,

115
00:06:04.585 --> 00:06:06.025
at the end of the day,

116
00:06:06.025 --> 00:06:08.050
aerodynamics and physics that

117
00:06:08.050 --> 00:06:11.260
govern the process of flying,

118
00:06:11.260 --> 00:06:14.710
and I do believe one day
we'll discover that.

119
00:06:14.710 --> 00:06:16.255
I sometimes think about this

120
00:06:16.255 --> 00:06:18.400
one learning
algorithm hypothesis.

121
00:06:18.400 --> 00:06:20.890
Could a lot of intelligence,
maybe not all,

122
00:06:20.890 --> 00:06:23.110
but a lot of it be explained by

123
00:06:23.110 --> 00:06:26.305
one or a very simple
machine learning principle?

124
00:06:26.305 --> 00:06:29.920
It feels like we're still so
far from cracking that nut.

125
00:06:29.920 --> 00:06:31.930
But in the weekends when I have

126
00:06:31.930 --> 00:06:33.250
spare time when I think about

127
00:06:33.250 --> 00:06:34.885
learning algorithms and
where they could go,

128
00:06:34.885 --> 00:06:36.790
this is one of the things I'm

129
00:06:36.790 --> 00:06:38.620
excited about. Just
thinking about that.

130
00:06:38.620 --> 00:06:41.320
I totally agree. I
still feel like we're

131
00:06:41.320 --> 00:06:44.125
pre-Newtonian if we're doing

132
00:06:44.125 --> 00:06:47.395
physics analogy before Newton.

133
00:06:47.395 --> 00:06:49.300
There has been great physics,

134
00:06:49.300 --> 00:06:52.480
great physicists, a
lot of phenomenology,

135
00:06:52.480 --> 00:06:54.190
a lot of studies of how

136
00:06:54.190 --> 00:06:57.580
the astral bodies
move and all that.

137
00:06:57.580 --> 00:07:01.750
But it was Newton who started
to write very simple laws.

138
00:07:01.750 --> 00:07:03.760
I think we are
still going through

139
00:07:03.760 --> 00:07:06.115
that very exciting coming of

140
00:07:06.115 --> 00:07:09.370
age of AI as a basic science.

141
00:07:09.370 --> 00:07:13.180
We're pre-Newton in my opinion.

142
00:07:13.180 --> 00:07:15.505
It's really nice to hear
you talk about how despite

143
00:07:15.505 --> 00:07:18.010
machine learning and
AI having come so far.

144
00:07:18.010 --> 00:07:19.525
It still feels like

145
00:07:19.525 --> 00:07:21.430
there are a lot more
unanswered questions,

146
00:07:21.430 --> 00:07:24.100
a lot more work to be
done by maybe some of

147
00:07:24.100 --> 00:07:25.240
the people joining the field

148
00:07:25.240 --> 00:07:27.460
today than work that's
already been done.

149
00:07:27.460 --> 00:07:30.280
Absolutely. Let's calculate,

150
00:07:30.280 --> 00:07:32.770
it's only 160 years about.

151
00:07:32.770 --> 00:07:34.900
It's a very nascent field.

152
00:07:34.900 --> 00:07:38.170
Modern physics and chemistry

153
00:07:38.170 --> 00:07:41.290
and biology are all
hundreds of years.

154
00:07:41.290 --> 00:07:47.560
I think it is very
exciting to be entering

155
00:07:47.560 --> 00:07:49.720
the field of science of

156
00:07:49.720 --> 00:07:53.635
intelligence and
studying AI today.

157
00:07:53.635 --> 00:07:54.805
Yeah, actually
that's good. I think

158
00:07:54.805 --> 00:07:55.870
I remember chatting with

159
00:07:55.870 --> 00:07:58.165
the late Professor John McCarthy

160
00:07:58.165 --> 00:08:00.970
who had coined the term
artificial intelligence,

161
00:08:00.970 --> 00:08:04.765
and boy, the field has
changed since when he

162
00:08:04.765 --> 00:08:06.160
conceived of it at

163
00:08:06.160 --> 00:08:09.055
a workshop and came
up with the term AI.

164
00:08:09.055 --> 00:08:11.620
But maybe another
ten years from now,

165
00:08:11.620 --> 00:08:14.530
maybe someone watching this
will come with a new set of

166
00:08:14.530 --> 00:08:17.215
ideas and then we'll be saying,

167
00:08:17.215 --> 00:08:18.850
boy, AI show us different than

168
00:08:18.850 --> 00:08:20.665
what you and I
thought it would be.

169
00:08:20.665 --> 00:08:22.585
That's the exciting
future to build towards.

170
00:08:22.585 --> 00:08:24.580
Yeah, I'm sure Newton would have

171
00:08:24.580 --> 00:08:27.115
not dreamed of Einstein.

172
00:08:27.115 --> 00:08:31.870
Our evolution of science
sometimes takes strides,

173
00:08:31.870 --> 00:08:33.280
sometimes takes a while,

174
00:08:33.280 --> 00:08:35.455
and I think we're absolutely

175
00:08:35.455 --> 00:08:40.160
in exciting phase
of AI right now.

176
00:08:40.470 --> 00:08:43.480
It's interesting hearing you

177
00:08:43.480 --> 00:08:45.745
paint this grand vision for AI.

178
00:08:45.745 --> 00:08:47.530
Going back a little
bit, there was one

179
00:08:47.530 --> 00:08:48.835
of the piece of your background

180
00:08:48.835 --> 00:08:51.820
that I found inspiring,

181
00:08:51.820 --> 00:08:54.820
which is when you're
just getting started,

182
00:08:54.820 --> 00:08:58.660
I've heard you speak about
how your physics student,

183
00:08:58.660 --> 00:09:00.760
but not only that, you're also

184
00:09:00.760 --> 00:09:04.255
running a laundromat
to pay for school.

185
00:09:04.255 --> 00:09:08.480
Just tell us more about that.

186
00:09:09.690 --> 00:09:12.595
I came to this country,

187
00:09:12.595 --> 00:09:15.895
to New Jersey actually
when I was 15.

188
00:09:15.895 --> 00:09:18.610
One thing great about
being in New Jersey is

189
00:09:18.610 --> 00:09:21.535
it was close to Princeton so I

190
00:09:21.535 --> 00:09:23.590
often just take a weekend trip

191
00:09:23.590 --> 00:09:25.840
with my parents and to admire

192
00:09:25.840 --> 00:09:28.210
the place where
Einstein spent most of

193
00:09:28.210 --> 00:09:31.600
his career in the latter
half of his life.

194
00:09:31.600 --> 00:09:37.060
But with typical immigrant
life and it was tough.

195
00:09:37.060 --> 00:09:39.385
By the time I enter Princeton,

196
00:09:39.385 --> 00:09:41.455
my parents didn't speak English

197
00:09:41.455 --> 00:09:45.090
and one thing led to another.

198
00:09:45.090 --> 00:09:47.460
It turns out running
a dry cleaner

199
00:09:47.460 --> 00:09:50.520
might be the best
option for my family,

200
00:09:50.520 --> 00:09:52.380
especially for me to lead

201
00:09:52.380 --> 00:09:54.935
that business because
it's a weekend business.

202
00:09:54.935 --> 00:09:56.575
If it's a weekday business,

203
00:09:56.575 --> 00:09:59.845
it would be hard for
me to be a student.

204
00:09:59.845 --> 00:10:02.215
It's actually, believe or not,

205
00:10:02.215 --> 00:10:05.380
running a dry cleaning shop
is very machine-heavy,

206
00:10:05.380 --> 00:10:09.515
which is good for a
STEM student like me.

207
00:10:09.515 --> 00:10:13.900
We decided to open
a dry cleaner shop

208
00:10:13.900 --> 00:10:15.990
in a small town

209
00:10:15.990 --> 00:10:18.390
in New Jersey called
Parsippany in New Jersey.

210
00:10:18.390 --> 00:10:21.250
It turned out we were physically

211
00:10:21.250 --> 00:10:24.025
not too far from Bell Labs

212
00:10:24.025 --> 00:10:25.600
and where lots of

213
00:10:25.600 --> 00:10:27.160
early convolutional neural

214
00:10:27.160 --> 00:10:30.160
network research was happening,

215
00:10:30.160 --> 00:10:32.100
but I had no idea.

216
00:10:32.100 --> 00:10:34.790
It's just summer
intern at the ancient.

217
00:10:34.790 --> 00:10:37.475
That is right, with Rob Shapiro.

218
00:10:37.475 --> 00:10:41.165
With Michael Kearns was my
mentor and Rob Shapiro,

219
00:10:41.165 --> 00:10:43.325
inventor of those things
creator algorithms.

220
00:10:43.325 --> 00:10:44.930
You're encoding AI.

221
00:10:44.930 --> 00:10:48.950
I was trying to [inaudible]

222
00:10:48.950 --> 00:10:51.800
Only much later that
I started interning.

223
00:10:51.800 --> 00:10:56.180
Yeah. Then it was seven years.

224
00:10:56.180 --> 00:10:59.960
I did that for the entire
undergrad and most

225
00:10:59.960 --> 00:11:04.235
of my grad school and I
hire my parents. Yeah.

226
00:11:04.235 --> 00:11:06.230
Yeah. Now, that's
really inspiring.

227
00:11:06.230 --> 00:11:08.150
I know you've been

228
00:11:08.150 --> 00:11:10.370
bred into doing exactly
where all your life.

229
00:11:10.370 --> 00:11:12.980
I think the story of running

230
00:11:12.980 --> 00:11:16.220
a laundromat to globally
prominent computer scientists.

231
00:11:16.220 --> 00:11:18.515
I hope that inspires

232
00:11:18.515 --> 00:11:21.590
some people watching this
that no matter where you are,

233
00:11:21.590 --> 00:11:23.915
there's plenty of for everyone.

234
00:11:23.915 --> 00:11:25.880
I don't even know this.

235
00:11:25.880 --> 00:11:27.635
My high-school job was

236
00:11:27.635 --> 00:11:31.100
office admin and so

237
00:11:31.100 --> 00:11:34.400
to this day I remember doing
a lot of photocopying.

238
00:11:34.400 --> 00:11:36.395
The exciting part was
using this shredder.

239
00:11:36.395 --> 00:11:38.195
That was a glamorous one.

240
00:11:38.195 --> 00:11:40.250
I was doing so much coffee in

241
00:11:40.250 --> 00:11:41.900
high school I
thought, boy, felony,

242
00:11:41.900 --> 00:11:44.810
I could build a robot to
do this for the coffee,

243
00:11:44.810 --> 00:11:46.145
maybe I could do something.

244
00:11:46.145 --> 00:11:47.840
Did you succeed?

245
00:11:47.840 --> 00:11:54.230
Still working on it. When people

246
00:11:54.230 --> 00:11:57.320
think about you and
the work you've done,

247
00:11:57.320 --> 00:12:00.500
one of the huge successes
everyone thinks about

248
00:12:00.500 --> 00:12:03.440
this ImageNet where help

249
00:12:03.440 --> 00:12:05.840
establish early benchmark
for computer vision.

250
00:12:05.840 --> 00:12:07.760
It was really completely
instrumental to

251
00:12:07.760 --> 00:12:10.880
the modern rise of deep
learning and computer vision.

252
00:12:10.880 --> 00:12:13.610
One thing I bet not many
people know about is

253
00:12:13.610 --> 00:12:16.670
how you actually got
started on ImageNet.

254
00:12:16.670 --> 00:12:20.135
Tell us the origin
story of ImageNet.

255
00:12:20.135 --> 00:12:21.725
Yeah. Well Andrew,

256
00:12:21.725 --> 00:12:23.930
that's a good question
because a lot of

257
00:12:23.930 --> 00:12:27.830
people see ImageNet as just
labeling a ton of images.

258
00:12:27.830 --> 00:12:30.740
But where we began was really

259
00:12:30.740 --> 00:12:32.435
going after a Northstar

260
00:12:32.435 --> 00:12:34.655
brings back my
physics background.

261
00:12:34.655 --> 00:12:36.170
When I enter grad school,

262
00:12:36.170 --> 00:12:38.405
when did you enter grad year?

263
00:12:38.405 --> 00:12:40.490
'97.

264
00:12:40.490 --> 00:12:44.045
I was three years
later that year, 2000.

265
00:12:44.045 --> 00:12:49.115
That was a very exciting
period because I was in

266
00:12:49.115 --> 00:12:52.160
computer vision and
computational neural science lab

267
00:12:52.160 --> 00:12:55.820
of Pietro Purana and
Christoph car at Caltech.

268
00:12:55.820 --> 00:12:58.355
Leading up to that,
there has been,

269
00:12:58.355 --> 00:13:01.175
first of all, two things
was very exciting.

270
00:13:01.175 --> 00:13:03.530
One is that the world

271
00:13:03.530 --> 00:13:06.920
of AI at that point
wasn't called AI.

272
00:13:06.920 --> 00:13:09.350
Computer vision or natural
language processing

273
00:13:09.350 --> 00:13:12.025
has founded Lingua Franca.

274
00:13:12.025 --> 00:13:14.920
It's machine
learning, statistical

275
00:13:14.920 --> 00:13:18.145
modeling as a new
tool has emerged.

276
00:13:18.145 --> 00:13:19.870
I mean, it's been around.

277
00:13:19.870 --> 00:13:21.980
I remember when the idea of

278
00:13:21.980 --> 00:13:24.455
applying machine learning
to computer vision,

279
00:13:24.455 --> 00:13:26.630
that was a controversial thing.

280
00:13:26.630 --> 00:13:29.180
I was the first generation of

281
00:13:29.180 --> 00:13:32.540
graduate students who were
embracing all the base net,

282
00:13:32.540 --> 00:13:36.095
all the inference
algorithms and all that.

283
00:13:36.095 --> 00:13:39.770
That was one exciting happening.

284
00:13:39.770 --> 00:13:42.095
A certainly exciting
happening that

285
00:13:42.095 --> 00:13:45.890
most people don't know
and don't appreciate is

286
00:13:45.890 --> 00:13:48.110
the couple of decades probably

287
00:13:48.110 --> 00:13:50.450
one of them two or
three decades of

288
00:13:50.450 --> 00:13:52.385
incredible cognitive science

289
00:13:52.385 --> 00:13:55.235
and cognitive neuroscience work

290
00:13:55.235 --> 00:13:56.780
in the field of vision,

291
00:13:56.780 --> 00:13:58.745
in the world of
vision, human vision.

292
00:13:58.745 --> 00:14:01.160
That has really
established a couple

293
00:14:01.160 --> 00:14:04.010
of really critical
Northstar problems.

294
00:14:04.010 --> 00:14:05.360
Just understanding of

295
00:14:05.360 --> 00:14:08.075
human visual processing
and human intelligence.

296
00:14:08.075 --> 00:14:10.355
One of them is the
recognition of

297
00:14:10.355 --> 00:14:14.120
understanding of natural
objects and natural things.

298
00:14:14.120 --> 00:14:15.890
Because a lot of

299
00:14:15.890 --> 00:14:18.620
the psychology and
cognitive science work

300
00:14:18.620 --> 00:14:20.255
is pointing to us.

301
00:14:20.255 --> 00:14:24.035
That is an innately optimized,

302
00:14:24.035 --> 00:14:26.090
whatever that word is.

303
00:14:26.090 --> 00:14:28.730
Functionality and the ability of

304
00:14:28.730 --> 00:14:34.250
human intelligence
is more robust,

305
00:14:34.250 --> 00:14:39.245
faster, and more nuanced
than we had thought.

306
00:14:39.245 --> 00:14:41.270
We even find neural correlates,

307
00:14:41.270 --> 00:14:48.005
brain areas devoted to faces
or places or body parts.

308
00:14:48.005 --> 00:14:52.730
These two things lead to
my PhD study of using

309
00:14:52.730 --> 00:14:55.910
machine-learning methods to work

310
00:14:55.910 --> 00:14:59.520
on real-world
objects recognition.

311
00:14:59.520 --> 00:15:02.560
But it becomes very painful very

312
00:15:02.560 --> 00:15:06.670
quickly that we
are coming banging

313
00:15:06.670 --> 00:15:11.600
against one of the
most continued to be

314
00:15:11.600 --> 00:15:14.750
the most important challenge in

315
00:15:14.750 --> 00:15:19.475
AI machine learning is the
lack of generalizability.

316
00:15:19.475 --> 00:15:22.850
You can design a beautiful model

317
00:15:22.850 --> 00:15:26.360
or you want if you're
overfitting the model.

318
00:15:26.360 --> 00:15:28.880
I remember when it used
be possible to publish

319
00:15:28.880 --> 00:15:30.680
a computer vision and paper

320
00:15:30.680 --> 00:15:32.690
showing it works on one image.

321
00:15:32.690 --> 00:15:33.810
Exactly.

322
00:15:33.810 --> 00:15:37.540
It's just the overfitting.

323
00:15:37.540 --> 00:15:39.715
The models are not
very expressive,

324
00:15:39.715 --> 00:15:43.210
and we lack the data.

325
00:15:43.210 --> 00:15:47.680
We also as a field was betting
on making the variables

326
00:15:47.680 --> 00:15:52.090
very rich by hand
engineered features.

327
00:15:52.090 --> 00:15:54.790
Remember, every
variable carrying

328
00:15:54.790 --> 00:15:56.590
a ton of semantic meaning,

329
00:15:56.590 --> 00:16:00.865
but with hand
engineered features.

330
00:16:00.865 --> 00:16:05.860
Then towards the end
of my PhD, my advisor,

331
00:16:05.860 --> 00:16:08.110
Pietro and I start to look
at each other and say,

332
00:16:08.110 --> 00:16:10.645
well, boy, we need more data.

333
00:16:10.645 --> 00:16:12.535
If we believe in

334
00:16:12.535 --> 00:16:16.105
this North Star problem
of object recognition,

335
00:16:16.105 --> 00:16:19.720
and we look back at
the tools we have,

336
00:16:19.720 --> 00:16:21.670
mathematically speaking,

337
00:16:21.670 --> 00:16:24.580
we're overfitting every
model we're encountering.

338
00:16:24.580 --> 00:16:27.100
We need to take a
fresh look at this.

339
00:16:27.100 --> 00:16:28.960
One thing led to another.

340
00:16:28.960 --> 00:16:32.470
He and I decided we'll
just do at that point.

341
00:16:32.470 --> 00:16:34.990
We think it was a
large-scale data project

342
00:16:34.990 --> 00:16:36.985
called Caltech 101.

343
00:16:36.985 --> 00:16:38.575
I remember the dataset.

344
00:16:38.575 --> 00:16:41.970
I wrote papers using your
Caltech 101 dataset way back.

345
00:16:41.970 --> 00:16:44.665
You did, you and your
early graduate student.

346
00:16:44.665 --> 00:16:46.210
You have benefit a
lot of researchers,

347
00:16:46.210 --> 00:16:48.220
that Caltech 101 dataset.

348
00:16:48.220 --> 00:16:52.095
That was me and my
mom labeling images,

349
00:16:52.095 --> 00:16:54.070
and a couple of undergrads,

350
00:16:54.070 --> 00:16:57.595
but it was the early
days of Internet.

351
00:16:57.595 --> 00:17:02.335
Suddenly the availability
of data was a new thing.

352
00:17:02.335 --> 00:17:04.660
I remember Pietro still have

353
00:17:04.660 --> 00:17:07.930
this super expensive
digital camera.

354
00:17:07.930 --> 00:17:10.060
I think it was Canon
or something like

355
00:17:10.060 --> 00:17:14.140
$6,000 walking around
Caltech taking pictures.

356
00:17:14.140 --> 00:17:17.860
But we're the
Internet generation.

357
00:17:17.860 --> 00:17:19.360
I go to Google image search,

358
00:17:19.360 --> 00:17:21.970
I start to see these thousands
and tens of thousands of

359
00:17:21.970 --> 00:17:25.255
images and I tell Pietro,
"Let's just download."

360
00:17:25.255 --> 00:17:27.730
Of course it's not
that easy to download.

361
00:17:27.730 --> 00:17:29.380
One thing led to another.

362
00:17:29.380 --> 00:17:32.290
We build this
Caltech 101 dataset

363
00:17:32.290 --> 00:17:34.675
of 101 object categories,

364
00:17:34.675 --> 00:17:41.200
and about 30,000 pictures.

365
00:17:41.200 --> 00:17:42.910
I think us really saying

366
00:17:42.910 --> 00:17:47.410
that even though everyone's
heard of ImageNet today,

367
00:17:47.410 --> 00:17:49.930
even you took a couple of

368
00:17:49.930 --> 00:17:51.190
iterations where you did

369
00:17:51.190 --> 00:17:53.155
Caltech 101 and
that was a success.

370
00:17:53.155 --> 00:17:55.090
Lots of people used it for

371
00:17:55.090 --> 00:17:58.390
even the early learnings
from building Caltech 101.

372
00:17:58.390 --> 00:18:00.520
They gave you the basis
to build what turned

373
00:18:00.520 --> 00:18:03.070
out to be an even
bigger success.

374
00:18:03.070 --> 00:18:09.405
Except that by the time I
became an assistant professor,

375
00:18:09.405 --> 00:18:11.670
we started to look
at the problem.

376
00:18:11.670 --> 00:18:13.950
I realized it's way
bigger than we think.

377
00:18:13.950 --> 00:18:17.290
Just mathematically
speaking, Caltech 101 was

378
00:18:17.290 --> 00:18:21.490
not sufficient to
power the algorithms.

379
00:18:21.490 --> 00:18:24.370
We decided to do image there.

380
00:18:24.370 --> 00:18:26.440
That was the time
people start to think

381
00:18:26.440 --> 00:18:28.690
we're doing too much.

382
00:18:28.690 --> 00:18:31.570
It's just too crazy,

383
00:18:31.570 --> 00:18:35.470
the idea of downloading the
entire Internet of images,

384
00:18:35.470 --> 00:18:40.870
mapping out all the English
nouns was a little bit.

385
00:18:40.870 --> 00:18:43.210
I start to get a
lot of push back.

386
00:18:43.210 --> 00:18:45.850
I remember at one of
the CVPR conference

387
00:18:45.850 --> 00:18:48.985
when I presented the
early idea of ImageNet,

388
00:18:48.985 --> 00:18:53.785
a couple of researchers
publicly questioned and said,

389
00:18:53.785 --> 00:18:57.805
"If you cannot recognize
one category of object,

390
00:18:57.805 --> 00:19:00.265
let's say the chair
you're sitting in,

391
00:19:00.265 --> 00:19:04.150
how do you imagine or what's
the use of a dataset of

392
00:19:04.150 --> 00:19:09.025
22,000 classes of
15 million images."

393
00:19:09.025 --> 00:19:13.300
In the end, that giant
dataset unlocked a lot of

394
00:19:13.300 --> 00:19:15.640
value for [inaudible] number

395
00:19:15.640 --> 00:19:17.590
of researchers around the world.

396
00:19:17.590 --> 00:19:21.160
I think it was the
combination of betting on

397
00:19:21.160 --> 00:19:26.500
the right North Star problem
and the data that drives it.

398
00:19:26.500 --> 00:19:28.660
It was a fun process.

399
00:19:28.660 --> 00:19:33.460
To me when I think
about that story,

400
00:19:33.460 --> 00:19:35.320
it seems like one of

401
00:19:35.320 --> 00:19:38.650
those examples where
sometimes people

402
00:19:38.650 --> 00:19:41.230
feel like they should
only work on projects

403
00:19:41.230 --> 00:19:43.900
without the huge thing
at the first outset.

404
00:19:43.900 --> 00:19:47.290
But I feel like for people
working in machine learning,

405
00:19:47.290 --> 00:19:48.640
if your first project is a

406
00:19:48.640 --> 00:19:50.245
bit smaller, it's totally fine.

407
00:19:50.245 --> 00:19:52.270
Have a good win.
Use the learning

408
00:19:52.270 --> 00:19:53.950
to build up to even
bigger things,

409
00:19:53.950 --> 00:19:55.060
and then sometimes you get

410
00:19:55.060 --> 00:19:58.375
the ImageNet size win all of it.

411
00:19:58.375 --> 00:20:00.730
But in the meantime,

412
00:20:00.730 --> 00:20:03.520
I think it's also
important to be

413
00:20:03.520 --> 00:20:07.180
driven by an audacious
goal though.

414
00:20:07.180 --> 00:20:10.990
You can size your problem
or size your project

415
00:20:10.990 --> 00:20:15.655
as local milestones and
so on along this journey,

416
00:20:15.655 --> 00:20:19.780
but I also look at some
of our current students.

417
00:20:19.780 --> 00:20:22.150
They're so pure pressured by

418
00:20:22.150 --> 00:20:27.190
this current climate
of publishing nonstop.

419
00:20:27.190 --> 00:20:29.800
It becomes more
incremental papers to

420
00:20:29.800 --> 00:20:33.970
just get into a publication
for the sake of it.

421
00:20:33.970 --> 00:20:38.440
I personally always push my
students to ask the question,

422
00:20:38.440 --> 00:20:40.555
what is the North Star
that's driving you?

423
00:20:40.555 --> 00:20:42.300
Yeah, that's true.

424
00:20:42.300 --> 00:20:45.035
Myself when I do
research over the years,

425
00:20:45.035 --> 00:20:49.325
I've always pretty much done
what I'm excited about,

426
00:20:49.325 --> 00:20:52.235
where I want to try to
push the view forward.

427
00:20:52.235 --> 00:20:53.450
Doesn't have to
listen to people.

428
00:20:53.450 --> 00:20:55.940
Have to listen to people let
them shape your opinion.

429
00:20:55.940 --> 00:20:57.320
But in the end, I think

430
00:20:57.320 --> 00:21:00.560
the best researchers let the
world shape their opinion,

431
00:21:00.560 --> 00:21:03.020
but in the end, drive things
for using their own opinion.

432
00:21:03.020 --> 00:21:04.175
Totally agree, yeah.

433
00:21:04.175 --> 00:21:06.450
It's your own fire.

434
00:21:07.000 --> 00:21:09.740
As a research program developed,

435
00:21:09.740 --> 00:21:11.900
you've wound up taking
your, let's say,

436
00:21:11.900 --> 00:21:15.860
foundations in computer vision
and neuroscience and apply

437
00:21:15.860 --> 00:21:17.300
it to all sorts of topics

438
00:21:17.300 --> 00:21:20.015
including your very
visibly health care.

439
00:21:20.015 --> 00:21:22.100
Looking at neuroscience
applications.

440
00:21:22.100 --> 00:21:24.470
We'd love to hear a
bit more about that.

441
00:21:24.470 --> 00:21:26.630
Yeah, happy to. I think

442
00:21:26.630 --> 00:21:29.330
the evolution of my
research in computer vision

443
00:21:29.330 --> 00:21:32.210
also follows the evolution

444
00:21:32.210 --> 00:21:35.015
of visual intelligence
in animals.

445
00:21:35.015 --> 00:21:38.165
There are two topics
that truly excites me.

446
00:21:38.165 --> 00:21:39.950
One is what is

447
00:21:39.950 --> 00:21:43.310
a truly impactful
application area

448
00:21:43.310 --> 00:21:45.590
that would help human lives?

449
00:21:45.590 --> 00:21:47.420
That's my health care work.

450
00:21:47.420 --> 00:21:49.820
The other one is what is

451
00:21:49.820 --> 00:21:52.880
vision at the end
of the day about?

452
00:21:52.880 --> 00:21:55.500
That brings me to

453
00:21:55.750 --> 00:21:58.520
trying to close the loop between

454
00:21:58.520 --> 00:22:01.010
perception and robotic learning.

455
00:22:01.010 --> 00:22:06.725
On the healthcare side,
one thing, Andrew,

456
00:22:06.725 --> 00:22:09.590
there was a number
that shocked me about

457
00:22:09.590 --> 00:22:12.995
10 years ago when I met my
long term collaborator,

458
00:22:12.995 --> 00:22:16.355
Dr. Arnie Milstein at
Stanford Medical School,

459
00:22:16.355 --> 00:22:19.340
and that number is
about a quarter of

460
00:22:19.340 --> 00:22:24.860
a million Americans die of
medical errors every year.

461
00:22:24.860 --> 00:22:27.305
I had never imagined

462
00:22:27.305 --> 00:22:30.515
a number being that high
due to medical errors.

463
00:22:30.515 --> 00:22:32.885
There are many reasons,

464
00:22:32.885 --> 00:22:35.270
but we can rest

465
00:22:35.270 --> 00:22:38.705
assure most of the reasons
are not intentional.

466
00:22:38.705 --> 00:22:40.985
These are errors of

467
00:22:40.985 --> 00:22:45.485
unintended mistakes and
solar, for example.

468
00:22:45.485 --> 00:22:47.150
That's a mind boggling number.

469
00:22:47.150 --> 00:22:47.960
It is.

470
00:22:47.960 --> 00:22:49.460
It's been about 40,000 deaths

471
00:22:49.460 --> 00:22:51.665
a year from
automotive accidents.

472
00:22:51.665 --> 00:22:54.440
It's just completely tragic
and this is even [inaudible]

473
00:22:54.440 --> 00:22:56.855
I was going to say that.
I'm glad you brought it up.

474
00:22:56.855 --> 00:22:58.580
Just one example.

475
00:22:58.580 --> 00:23:01.490
One number within that
mind-boggling number

476
00:23:01.490 --> 00:23:03.620
is the number of
hospital acquired

477
00:23:03.620 --> 00:23:10.535
infection resulted fatality
is more than 95,000.

478
00:23:10.535 --> 00:23:16.620
That's 2.5 times than the
death of car accidents.

479
00:23:16.870 --> 00:23:19.235
In this particular case,

480
00:23:19.235 --> 00:23:22.100
hospital acquired infection
as a result of many things.

481
00:23:22.100 --> 00:23:27.890
But in enlarge, lack of
good hand hygiene practice.

482
00:23:27.890 --> 00:23:30.275
If you look at WHO,

483
00:23:30.275 --> 00:23:32.600
there has been a
lot of protocols

484
00:23:32.600 --> 00:23:34.910
about clinicians hand
hygiene practice.

485
00:23:34.910 --> 00:23:38.190
But in real health
care delivery,

486
00:23:38.350 --> 00:23:42.650
when things get busy
and when the process

487
00:23:42.650 --> 00:23:46.160
is tedious and when there's
a lack of feedback system,

488
00:23:46.160 --> 00:23:48.095
you still make a
lot of mistakes.

489
00:23:48.095 --> 00:23:52.940
Another tragic
medical fact is that

490
00:23:52.940 --> 00:23:56.375
more than $70 billion
every year are spent

491
00:23:56.375 --> 00:24:01.190
in full resulted
injuries and fatalities.

492
00:24:01.190 --> 00:24:03.980
Most of this happen
to elderlies at home,

493
00:24:03.980 --> 00:24:05.930
but also in the hospital rooms.

494
00:24:05.930 --> 00:24:08.360
These are huge issues.

495
00:24:08.360 --> 00:24:12.470
When Arnie and I got
together back in 2012,

496
00:24:12.470 --> 00:24:16.130
it was the height of
self-driving car,

497
00:24:16.130 --> 00:24:18.410
let's say not hype.

498
00:24:18.410 --> 00:24:20.840
But what's the right word?

499
00:24:20.840 --> 00:24:23.255
Excitement in Silicon Valley.

500
00:24:23.255 --> 00:24:28.340
Then we look at the technology
of smart sensing cameras,

501
00:24:28.340 --> 00:24:32.570
lidars, whatever, smart sensors,

502
00:24:32.570 --> 00:24:37.175
machine-learning algorithm,
and holistic understanding

503
00:24:37.175 --> 00:24:39.485
of a complex environment

504
00:24:39.485 --> 00:24:42.620
with high-stakes
for human lives.

505
00:24:42.620 --> 00:24:45.800
I was looking at all that
for self-driving car and

506
00:24:45.800 --> 00:24:49.355
realized in healthcare delivery,

507
00:24:49.355 --> 00:24:51.650
we have the same situation.

508
00:24:51.650 --> 00:24:53.270
Much of the process,

509
00:24:53.270 --> 00:24:58.415
the human behavior process of
health care is in the dark.

510
00:24:58.415 --> 00:25:03.335
If we could have smart sensors
be it in patient rooms or

511
00:25:03.335 --> 00:25:05.270
senior homes to help

512
00:25:05.270 --> 00:25:08.360
our clinicians and
patients to stay safer,

513
00:25:08.360 --> 00:25:09.755
that will be amazing.

514
00:25:09.755 --> 00:25:11.750
Arnie and I embarked on this,

515
00:25:11.750 --> 00:25:15.845
what we call ambient
intelligence research agenda.

516
00:25:15.845 --> 00:25:18.230
But one thing I learned
which probably will

517
00:25:18.230 --> 00:25:20.885
lead to our other topics,

518
00:25:20.885 --> 00:25:23.660
is as soon as you're applying

519
00:25:23.660 --> 00:25:26.735
AI to real human conditions,

520
00:25:26.735 --> 00:25:28.655
there's a lot of human issues

521
00:25:28.655 --> 00:25:30.740
in addition to machine
learning issues,

522
00:25:30.740 --> 00:25:32.450
for example, privacy.

523
00:25:32.450 --> 00:25:33.920
I remember reading some of

524
00:25:33.920 --> 00:25:36.710
your papers with Arnie
and found it really

525
00:25:36.710 --> 00:25:38.810
interesting how you could
build and deploy systems that

526
00:25:38.810 --> 00:25:41.570
were relatively
privacy preserving.

527
00:25:41.570 --> 00:25:42.905
Yeah, well, thank you. Well,

528
00:25:42.905 --> 00:25:46.690
the first iteration
of that technology

529
00:25:46.690 --> 00:25:50.800
is we use cameras that do
not capture RGB information.

530
00:25:50.800 --> 00:25:53.020
You've used a lot of that
in self-driving car,

531
00:25:53.020 --> 00:25:55.020
that depth cameras, for example.

532
00:25:55.020 --> 00:25:58.100
There you preserve a lot of

533
00:25:58.100 --> 00:26:01.655
privacy information just by

534
00:26:01.655 --> 00:26:04.920
not seeing the faces and
the identity of the people.

535
00:26:04.920 --> 00:26:06.790
But what's really
interesting over

536
00:26:06.790 --> 00:26:08.950
the past decade
is the changes of

537
00:26:08.950 --> 00:26:11.320
technology is actually giving

538
00:26:11.320 --> 00:26:14.305
us a bigger toolset for privacy,

539
00:26:14.305 --> 00:26:17.710
preserved, a computing
in this condition,

540
00:26:17.710 --> 00:26:21.960
for example,
on-device inference.

541
00:26:21.960 --> 00:26:24.635
As the chips getting
more and more powerful,

542
00:26:24.635 --> 00:26:27.080
if you don't have to
transmit any data

543
00:26:27.080 --> 00:26:29.645
through the network and
to the central server,

544
00:26:29.645 --> 00:26:31.145
you help people better.

545
00:26:31.145 --> 00:26:35.120
Federated learning, we know
it's still early stage,

546
00:26:35.120 --> 00:26:38.135
but that's another
potential tool

547
00:26:38.135 --> 00:26:40.550
for privacy,
preserved computing.

548
00:26:40.550 --> 00:26:42.275
Then differential privacy

549
00:26:42.275 --> 00:26:45.815
and also encryption
technologies.

550
00:26:45.815 --> 00:26:49.925
We're starting to see
that human demand,

551
00:26:49.925 --> 00:26:54.380
privacy and other issues is
driving actually a new wave

552
00:26:54.380 --> 00:26:56.285
of machine learning technology

553
00:26:56.285 --> 00:26:59.255
in ambient intelligence
in health care.

554
00:26:59.255 --> 00:27:01.715
Yeah, I've been
encouraged to see

555
00:27:01.715 --> 00:27:04.565
your real practical applications

556
00:27:04.565 --> 00:27:07.550
of differential privacy
that are actually real.

557
00:27:07.550 --> 00:27:09.230
Federated Learning as you said,

558
00:27:09.230 --> 00:27:11.090
Pray the PRRs little bit

559
00:27:11.090 --> 00:27:12.995
ahead of the reality, but
I think we'll get there.

560
00:27:12.995 --> 00:27:15.200
But it's interesting
how consumers

561
00:27:15.200 --> 00:27:17.180
in the last several years have,

562
00:27:17.180 --> 00:27:19.130
fortunately, gotten much more

563
00:27:19.130 --> 00:27:21.950
knowledgeable about
privacy and increasingly.

564
00:27:21.950 --> 00:27:23.630
I think the public is

565
00:27:23.630 --> 00:27:26.960
also making us to be
better scientist.

566
00:27:26.960 --> 00:27:29.555
Yeah, I think

567
00:27:29.555 --> 00:27:32.915
ultimately people understand
the AI hosts everyone,

568
00:27:32.915 --> 00:27:34.280
including us, but holds everyone

569
00:27:34.280 --> 00:27:37.085
accountable for really
doing the right thing.

570
00:27:37.085 --> 00:27:38.640
Yeah.

571
00:27:39.040 --> 00:27:42.260
On that note, one of

572
00:27:42.260 --> 00:27:43.820
the really interesting piece

573
00:27:43.820 --> 00:27:45.020
of work you've been doing has

574
00:27:45.020 --> 00:27:50.840
been leading several efforts
to help educate legislators.

575
00:27:50.840 --> 00:27:54.380
I hope governments,
especially US government,

576
00:27:54.380 --> 00:27:57.500
work through better laws
and better regulation,

577
00:27:57.500 --> 00:27:59.690
especially as it relates to AI.

578
00:27:59.690 --> 00:28:02.135
That sounds a very important

579
00:28:02.135 --> 00:28:04.460
and I suspect some
days they'll be,

580
00:28:04.460 --> 00:28:06.230
I would guess, somewhat
frustrating work,

581
00:28:06.230 --> 00:28:08.015
but we'd love to hear
more about that.

582
00:28:08.015 --> 00:28:10.220
Yeah, first of all,

583
00:28:10.220 --> 00:28:12.470
I have to credit
many, many people.

584
00:28:12.470 --> 00:28:15.590
About four years ago,

585
00:28:15.590 --> 00:28:19.640
I was actually finishing my
sabbatical from Google time.

586
00:28:19.640 --> 00:28:23.195
I was very privileged to work
with so many businesses.

587
00:28:23.195 --> 00:28:28.745
Enterprise developers,
just a large

588
00:28:28.745 --> 00:28:31.145
number and variety of

589
00:28:31.145 --> 00:28:35.330
vertical industries are
realizing AI's human impact.

590
00:28:35.330 --> 00:28:39.200
That was one, meaning
faculty leaders at

591
00:28:39.200 --> 00:28:43.130
Stanford and also just
our president, provost,

592
00:28:43.130 --> 00:28:44.870
former President and former

593
00:28:44.870 --> 00:28:47.435
provost all get together
and realize there is

594
00:28:47.435 --> 00:28:50.660
a historical role
that Stanford needs

595
00:28:50.660 --> 00:28:54.290
to play in advances of AI.

596
00:28:54.290 --> 00:28:59.135
We were part of the
birth place of AI.

597
00:28:59.135 --> 00:29:04.580
A lot of work, our previous
generation have done and

598
00:29:04.580 --> 00:29:07.100
all of work you've done
and some of the work

599
00:29:07.100 --> 00:29:10.190
I've done lead to today's AI,

600
00:29:10.190 --> 00:29:14.330
what is our historical
opportunity and responsibility?

601
00:29:14.330 --> 00:29:18.110
With that, we believe that

602
00:29:18.110 --> 00:29:20.720
the next generation
of AI education

603
00:29:20.720 --> 00:29:24.410
and research and policy
needs to be human centered.

604
00:29:24.410 --> 00:29:28.520
Having established the
humans center AI institute,

605
00:29:28.520 --> 00:29:30.530
what we call HAI,

606
00:29:30.530 --> 00:29:32.930
one of the work that really took

607
00:29:32.930 --> 00:29:35.780
me outside of my comfort
zone were aiming

608
00:29:35.780 --> 00:29:41.330
expertise is really
a deeper engagement

609
00:29:41.330 --> 00:29:44.495
with policy thinkers and makers.

610
00:29:44.495 --> 00:29:46.400
Because we're here in

611
00:29:46.400 --> 00:29:48.620
Silicon Valley and
there is a culture in

612
00:29:48.620 --> 00:29:50.510
Silicon Valley is
we just keep making

613
00:29:50.510 --> 00:29:53.330
things and the law will
catch up by itself.

614
00:29:53.330 --> 00:29:58.820
But AI is impacting human
lives and sometimes negatively

615
00:29:58.820 --> 00:30:04.460
so rapidly that it is
not good for any of us.

616
00:30:04.460 --> 00:30:06.905
If we, the experts,

617
00:30:06.905 --> 00:30:10.070
are not at the table with
a policy thinkers and

618
00:30:10.070 --> 00:30:11.930
makers to really try to make

619
00:30:11.930 --> 00:30:14.000
this technology better
for the people.

620
00:30:14.000 --> 00:30:15.725
We're talking about fairness,

621
00:30:15.725 --> 00:30:17.375
we're talking about privacy,

622
00:30:17.375 --> 00:30:19.790
we also are talking about

623
00:30:19.790 --> 00:30:23.540
the brain drain of
AI to industry and

624
00:30:23.540 --> 00:30:27.715
the concentration
of data and compute

625
00:30:27.715 --> 00:30:32.455
in a small number of
technology companies.

626
00:30:32.455 --> 00:30:37.100
All these are really part
of the changes of our time.

627
00:30:37.100 --> 00:30:38.825
Some are really
exciting changes,

628
00:30:38.825 --> 00:30:41.555
some have profoundly
impact that we

629
00:30:41.555 --> 00:30:45.290
cannot necessarily
predicting yet.

630
00:30:45.290 --> 00:30:47.420
One of the policy work that

631
00:30:47.420 --> 00:30:50.630
Stanford AI has very
proudly engaged in,

632
00:30:50.630 --> 00:30:54.950
is we were one of the leading
universities that lobbied

633
00:30:54.950 --> 00:30:56.930
a bill called the

634
00:30:56.930 --> 00:31:00.995
National AI Research
Cloud Task Force Bill.

635
00:31:00.995 --> 00:31:02.780
It changed the name from

636
00:31:02.780 --> 00:31:05.165
research Cloud to
research resource.

637
00:31:05.165 --> 00:31:07.805
Now the bill's acronym is NAIR,

638
00:31:07.805 --> 00:31:10.085
National AI Research Resource.

639
00:31:10.085 --> 00:31:13.970
This bill is calling
for a task force to put

640
00:31:13.970 --> 00:31:18.050
together a road-map for
America's public sector,

641
00:31:18.050 --> 00:31:19.640
especially higher education,

642
00:31:19.640 --> 00:31:23.360
and research sector to

643
00:31:23.360 --> 00:31:27.605
increase their access to
resource for AI compute and

644
00:31:27.605 --> 00:31:32.150
AI data and really is
aimed to rejuvenate

645
00:31:32.150 --> 00:31:37.850
America's ecosystem in AI
innovation and research.

646
00:31:37.850 --> 00:31:41.640
I'm on the 12-person task-force

647
00:31:41.830 --> 00:31:45.170
under Biden administration
for this bill,

648
00:31:45.170 --> 00:31:47.450
and we hope that's
a piece of policy

649
00:31:47.450 --> 00:31:50.450
that is not a regulatory policy,

650
00:31:50.450 --> 00:31:52.910
it's more an incentive policy

651
00:31:52.910 --> 00:31:55.970
to build and
rejuvenate ecosystems.

652
00:31:55.970 --> 00:31:57.530
I'm glad that you're doing this,

653
00:31:57.530 --> 00:31:59.000
The Help Shape US Policy,

654
00:31:59.000 --> 00:32:02.000
and making sure
enough resources are

655
00:32:02.000 --> 00:32:05.240
allocated to ensure
healthy development of AI.

656
00:32:05.240 --> 00:32:06.650
I feel like this
is something that

657
00:32:06.650 --> 00:32:09.065
every country needs
at this point.

658
00:32:09.065 --> 00:32:10.190
Yeah.

659
00:32:10.190 --> 00:32:14.780
Just from the things that
you are doing by yourself,

660
00:32:14.780 --> 00:32:16.310
not to speak of the things that

661
00:32:16.310 --> 00:32:18.620
the Global AI
Community is doing,

662
00:32:18.620 --> 00:32:20.750
there's just so much
going on in AI right

663
00:32:20.750 --> 00:32:23.450
now so many opportunities,
so much excitement.

664
00:32:23.450 --> 00:32:26.150
I found that for someone

665
00:32:26.150 --> 00:32:28.700
getting started in machine
learning for the first time,

666
00:32:28.700 --> 00:32:30.560
sometimes there's so much
going on and they can

667
00:32:30.560 --> 00:32:33.050
almost feel a little
bit overwhelming.

668
00:32:33.050 --> 00:32:33.830
Totally.

669
00:32:33.830 --> 00:32:35.930
What advice do you have for

670
00:32:35.930 --> 00:32:38.795
someone getting started
in machine learning?

671
00:32:38.795 --> 00:32:42.020
Good question, Andrew, I'm
sure you have great advice.

672
00:32:42.020 --> 00:32:45.140
You're one of the
world-known advocate

673
00:32:45.140 --> 00:32:47.930
for AI machine
learning education.

674
00:32:47.930 --> 00:32:51.200
I do get this question
a lot as well,

675
00:32:51.200 --> 00:32:53.330
and one thing you're
totally right

676
00:32:53.330 --> 00:32:57.920
is AI really today feels
different from our time.

677
00:32:57.920 --> 00:33:01.835
Just for the record, you
all are still on time.

678
00:33:01.835 --> 00:33:07.880
That's true when we
were starting in AI,

679
00:33:07.880 --> 00:33:11.225
I love that exactly we're
still part of this.

680
00:33:11.225 --> 00:33:14.060
When we first started
the entrance to

681
00:33:14.060 --> 00:33:16.580
AI and machine learning
was relatively narrow.

682
00:33:16.580 --> 00:33:22.025
You almost have to start from
computer science and go.

683
00:33:22.025 --> 00:33:23.810
As a physics major,

684
00:33:23.810 --> 00:33:25.880
I still had to wedge myself into

685
00:33:25.880 --> 00:33:28.010
the computer science track or

686
00:33:28.010 --> 00:33:31.700
electrical engineering
track to get to AI.

687
00:33:31.700 --> 00:33:36.650
But today, I actually think
that there is many aspect of

688
00:33:36.650 --> 00:33:39.830
AI that creates entry points

689
00:33:39.830 --> 00:33:42.560
for people from
all walks of life.

690
00:33:42.560 --> 00:33:44.720
On the technical side,

691
00:33:44.720 --> 00:33:48.110
I think it's obvious
that there's

692
00:33:48.110 --> 00:33:51.395
just incredible plethora of

693
00:33:51.395 --> 00:33:53.690
resources out there
on the Internet

694
00:33:53.690 --> 00:33:56.120
from Coursera to YouTube,

695
00:33:56.120 --> 00:34:01.880
to TikTok there's just
so much that students

696
00:34:01.880 --> 00:34:05.180
worldwide can learn about AI and

697
00:34:05.180 --> 00:34:06.725
machine learning compared to

698
00:34:06.725 --> 00:34:09.470
the time we began learning
machine learning.

699
00:34:09.470 --> 00:34:11.420
Also any campuses,

700
00:34:11.420 --> 00:34:13.640
we're not talking about
just college campuses

701
00:34:13.640 --> 00:34:15.440
we're talking about
high school campuses,

702
00:34:15.440 --> 00:34:18.560
or even sometimes earlier,

703
00:34:18.560 --> 00:34:20.090
we're starting to see

704
00:34:20.090 --> 00:34:25.100
more available classes
and resources.

705
00:34:25.100 --> 00:34:28.580
I do encourage those

706
00:34:28.580 --> 00:34:32.030
of the young people with
a technical interest

707
00:34:32.030 --> 00:34:35.870
and resource and opportunity to

708
00:34:35.870 --> 00:34:40.565
embrace these resources
because it's a lot of fun.

709
00:34:40.565 --> 00:34:42.665
But having said that,

710
00:34:42.665 --> 00:34:44.420
for those of you who are not

711
00:34:44.420 --> 00:34:46.115
coming from a technical angle,

712
00:34:46.115 --> 00:34:48.785
who still are
passionate about AI,

713
00:34:48.785 --> 00:34:51.200
whether it's the
downstream application

714
00:34:51.200 --> 00:34:55.085
or the creativity it engenders,

715
00:34:55.085 --> 00:34:58.415
or the policy and social angle,

716
00:34:58.415 --> 00:35:00.560
or important social problems,

717
00:35:00.560 --> 00:35:02.990
whether it's digital
economics or

718
00:35:02.990 --> 00:35:06.665
the governance or history,

719
00:35:06.665 --> 00:35:10.505
ethics, political sciences,

720
00:35:10.505 --> 00:35:14.150
I do invite you to join us

721
00:35:14.150 --> 00:35:17.300
because there is a lot
of work to be done.

722
00:35:17.300 --> 00:35:22.385
There's a lot of unknown
questions, for example,

723
00:35:22.385 --> 00:35:27.080
my colleague at HAI are
trying to find answers

724
00:35:27.080 --> 00:35:32.420
on how do you define our
economy in the digital age?

725
00:35:32.420 --> 00:35:34.820
What does it mean when robots,

726
00:35:34.820 --> 00:35:36.230
or software, are

727
00:35:36.230 --> 00:35:39.200
participating in the
workflow more and more?

728
00:35:39.200 --> 00:35:42.170
How do you measure our economy?

729
00:35:42.170 --> 00:35:47.390
That's not AI coding question
that is AI impact question.

730
00:35:47.390 --> 00:35:50.390
We're looking at the
incredible advances

731
00:35:50.390 --> 00:35:53.355
of generative AI and
there will be more.

732
00:35:53.355 --> 00:35:56.685
What does that mean
for creativity,

733
00:35:56.685 --> 00:36:01.130
and to the creators from
music to art, to writing?

734
00:36:01.130 --> 00:36:04.940
I think there is a
lot of concerns,

735
00:36:04.940 --> 00:36:06.830
and I think it's rightfully so,

736
00:36:06.830 --> 00:36:08.705
but in the meantime,

737
00:36:08.705 --> 00:36:11.525
it takes people
together to figure this

738
00:36:11.525 --> 00:36:15.240
out and also to
use this new tool.

739
00:36:15.250 --> 00:36:19.730
In short, I just think
it's a very exciting time,

740
00:36:19.730 --> 00:36:22.835
and anybody with
any walks of life,

741
00:36:22.835 --> 00:36:24.890
as long as you are
passionate about this,

742
00:36:24.890 --> 00:36:26.555
there's a role to play.

743
00:36:26.555 --> 00:36:29.105
That's really exciting when
you talk about economics,

744
00:36:29.105 --> 00:36:30.860
think about my conversations

745
00:36:30.860 --> 00:36:33.735
with Professor
Erik Brynjolfsson.

746
00:36:33.735 --> 00:36:35.920
Impact of AI on the economy,

747
00:36:35.920 --> 00:36:38.710
but from what you're
saying and I agree,

748
00:36:38.710 --> 00:36:42.700
it seems like no matter what
your current interests are,

749
00:36:42.700 --> 00:36:46.195
AI is such a general-purpose
technology that

750
00:36:46.195 --> 00:36:48.145
the combination of your
current interest in

751
00:36:48.145 --> 00:36:50.935
AI is often promising.

752
00:36:50.935 --> 00:36:53.290
I find that even for learners

753
00:36:53.290 --> 00:36:56.020
that may not yet have
a specific interest,

754
00:36:56.020 --> 00:36:57.850
if you find your way into AI,

755
00:36:57.850 --> 00:36:59.290
start learning things,

756
00:36:59.290 --> 00:37:01.870
often the interest will
evolve and then you can

757
00:37:01.870 --> 00:37:04.600
start to craft your own path.

758
00:37:04.600 --> 00:37:06.505
Given way AI is today,

759
00:37:06.505 --> 00:37:09.040
there's still so much
room and so much need for

760
00:37:09.040 --> 00:37:11.935
a lot more people to
craft their own path,

761
00:37:11.935 --> 00:37:13.960
to do this exciting work that I

762
00:37:13.960 --> 00:37:16.000
think the world still
needs a lot more of.

763
00:37:16.000 --> 00:37:17.380
Totally agree.

764
00:37:17.380 --> 00:37:19.270
One piece of work that you did

765
00:37:19.270 --> 00:37:20.800
I thought was very
cool was starting

766
00:37:20.800 --> 00:37:22.360
a program initially called

767
00:37:22.360 --> 00:37:24.880
SAILORS and then later AI4ALL,

768
00:37:24.880 --> 00:37:26.560
which was really reaching out

769
00:37:26.560 --> 00:37:29.320
to high school and
even younger students,

770
00:37:29.320 --> 00:37:32.185
to try to give them more
opportunities in AI,

771
00:37:32.185 --> 00:37:34.270
including people of
all walks of life.

772
00:37:34.270 --> 00:37:36.130
I'd love to hear
more about that.

773
00:37:36.130 --> 00:37:39.085
This is in the spirit
of this conversation

774
00:37:39.085 --> 00:37:43.795
is that was back in 2015.

775
00:37:43.795 --> 00:37:49.270
There was starting to be a
lot of excitement of AI,

776
00:37:49.270 --> 00:37:52.240
but there was also
starting to be this talk

777
00:37:52.240 --> 00:37:58.075
about killer robot coming next
door, terminators coming.

778
00:37:58.075 --> 00:38:00.745
At that time Andrew,

779
00:38:00.745 --> 00:38:02.320
I was the director of

780
00:38:02.320 --> 00:38:05.050
Stanford AI Lab,
and I was thinking,

781
00:38:05.050 --> 00:38:09.340
we know how far we are from
terminators coming and

782
00:38:09.340 --> 00:38:13.375
that seemed to be a little
bit far-fetched concern.

783
00:38:13.375 --> 00:38:16.255
But I was living my work life

784
00:38:16.255 --> 00:38:19.255
with a real concern I felt
no one was talking about,

785
00:38:19.255 --> 00:38:22.240
which was the lack of
representation in AI.

786
00:38:22.240 --> 00:38:25.150
At that time, I guess
after Daphne has left,

787
00:38:25.150 --> 00:38:29.690
I was the only woman
faculty at Stanford AI Lab.

788
00:38:29.730 --> 00:38:32.815
We're having very small,

789
00:38:32.815 --> 00:38:36.220
around 15% of women
graduate students,

790
00:38:36.220 --> 00:38:39.415
and we really don't
see anybody from

791
00:38:39.415 --> 00:38:41.950
the underrepresented
minority groups

792
00:38:41.950 --> 00:38:44.980
in Stanford AI program.

793
00:38:44.980 --> 00:38:48.160
This is a national or
even worldwide issue,

794
00:38:48.160 --> 00:38:50.275
so it wasn't just Stanford.

795
00:38:50.275 --> 00:38:52.390
Frankly, it still needs
a lot of work today.

796
00:38:52.390 --> 00:38:54.490
Exactly. How do we do this?

797
00:38:54.490 --> 00:38:56.050
Well, I got together with

798
00:38:56.050 --> 00:38:58.660
my former student
Ugawa Sakofski,

799
00:38:58.660 --> 00:39:02.790
and also a long-term educator of

800
00:39:02.790 --> 00:39:06.330
STEM topics Doctor Rick Summer

801
00:39:06.330 --> 00:39:10.230
from Stanford Pre-Collegiate
Study Program,

802
00:39:10.230 --> 00:39:12.510
and thought about inviting

803
00:39:12.510 --> 00:39:14.920
high schoolers at
that time women,

804
00:39:14.920 --> 00:39:17.650
high-school young
women to participate

805
00:39:17.650 --> 00:39:21.190
in a summer program to
inspire them to learn AI,

806
00:39:21.190 --> 00:39:24.850
and that was how it
started in 2015,

807
00:39:24.850 --> 00:39:29.380
and 2017 we got a
lot of encouragement

808
00:39:29.380 --> 00:39:31.495
and support from people like

809
00:39:31.495 --> 00:39:34.615
Jensen and Lori Hung
and Melinda Gates,

810
00:39:34.615 --> 00:39:38.740
and we formed a national
non-profit AI4ALL,

811
00:39:38.740 --> 00:39:43.480
which is really
committed to training or

812
00:39:43.480 --> 00:39:49.060
shaping tomorrow's
leaders for AI.

813
00:39:49.060 --> 00:39:51.565
From students of
all walks of life,

814
00:39:51.565 --> 00:39:53.275
especially the traditionally

815
00:39:53.275 --> 00:39:57.110
under-served and
underrepresented communities.

816
00:39:58.080 --> 00:40:00.610
Till today, we've had

817
00:40:00.610 --> 00:40:04.180
many summer camps
and summer programs

818
00:40:04.180 --> 00:40:05.290
across the country

819
00:40:05.290 --> 00:40:10.285
more than 15 universities
are involved,

820
00:40:10.285 --> 00:40:13.870
and we have online curriculum

821
00:40:13.870 --> 00:40:16.660
to encourage students
as well as college

822
00:40:16.660 --> 00:40:19.930
pathway programs to
continue support

823
00:40:19.930 --> 00:40:22.690
these students' career by

824
00:40:22.690 --> 00:40:25.960
matching them with
internships and mentors.

825
00:40:25.960 --> 00:40:28.570
It's a continuous effort of

826
00:40:28.570 --> 00:40:31.555
encouraging students
of all walks of life.

827
00:40:31.555 --> 00:40:33.115
I remember back then,

828
00:40:33.115 --> 00:40:34.420
I think your group was printing

829
00:40:34.420 --> 00:40:37.195
these really cool t-shirts
that asked the question.

830
00:40:37.195 --> 00:40:39.070
AI will change the world,

831
00:40:39.070 --> 00:40:40.555
who will change AI?

832
00:40:40.555 --> 00:40:42.595
I thought the answer
of making sure

833
00:40:42.595 --> 00:40:44.920
everyone can come
in and participate,

834
00:40:44.920 --> 00:40:46.225
that was a great answer.

835
00:40:46.225 --> 00:40:49.550
Still an important
question today.

836
00:40:49.550 --> 00:40:52.725
That's a great thought
and I think that

837
00:40:52.725 --> 00:40:55.740
takes us toward the
end of the interview.

838
00:40:55.740 --> 00:40:58.680
Any final thoughts for
the people watching this?

839
00:40:58.680 --> 00:41:02.980
Still, that this is a
very nascent field.

840
00:41:02.980 --> 00:41:04.150
As you said, Andrew,

841
00:41:04.150 --> 00:41:05.815
we're still in the
middle of this.

842
00:41:05.815 --> 00:41:08.290
I still feel there's
just so many questions

843
00:41:08.290 --> 00:41:10.060
that I wake up

844
00:41:10.060 --> 00:41:12.910
excited to work on with
my students in the lab,

845
00:41:12.910 --> 00:41:14.530
and I think there's

846
00:41:14.530 --> 00:41:17.350
a lot more opportunities
for the young people

847
00:41:17.350 --> 00:41:18.865
out there who want to learn and

848
00:41:18.865 --> 00:41:22.480
contribute and shape
tomorrow's AI.

849
00:41:22.480 --> 00:41:25.570
Well said [inaudible]
that's very inspiring,

850
00:41:25.570 --> 00:41:27.370
really great to chat with you,

851
00:41:27.370 --> 00:41:28.685
and thank you for
attending this video.

852
00:41:28.685 --> 00:41:33.170
Thank you. It's fun to
have these conversations.