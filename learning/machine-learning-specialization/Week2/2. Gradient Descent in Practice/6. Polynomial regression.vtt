WEBVTT

1
00:00:02.880 --> 00:00:05.050
So far we've just been

2
00:00:05.050 --> 00:00:07.070
fitting straight
lines to our data.

3
00:00:07.070 --> 00:00:10.450
Let's take the ideas of
multiple linear regression and

4
00:00:10.450 --> 00:00:12.460
feature engineering
to come up with

5
00:00:12.460 --> 00:00:15.055
a new algorithm called
polynomial regression,

6
00:00:15.055 --> 00:00:16.615
which will let you fit curves,

7
00:00:16.615 --> 00:00:18.780
non-linear functions,
to your data.

8
00:00:18.780 --> 00:00:20.420
Let's say you have a housing

9
00:00:20.420 --> 00:00:22.420
data-set that looks like this,

10
00:00:22.420 --> 00:00:25.915
where feature x is the
size in square feet.

11
00:00:25.915 --> 00:00:27.735
It doesn't look like
a straight line

12
00:00:27.735 --> 00:00:29.980
fits this data-set very well.

13
00:00:29.980 --> 00:00:32.095
Maybe you want to fit a curve,

14
00:00:32.095 --> 00:00:35.785
maybe a quadratic
function to the data like

15
00:00:35.785 --> 00:00:41.005
this which includes a size
x and also x squared,

16
00:00:41.005 --> 00:00:44.095
which is the size raised
to the power of two.

17
00:00:44.095 --> 00:00:47.570
Maybe that will give you
a better fit to the data.

18
00:00:47.570 --> 00:00:49.200
But then you may decide that

19
00:00:49.200 --> 00:00:51.355
your quadratic model
doesn't really make sense

20
00:00:51.355 --> 00:00:55.115
because a quadratic function
eventually comes back down.

21
00:00:55.115 --> 00:00:56.980
Well, we wouldn't really expect

22
00:00:56.980 --> 00:01:00.505
housing prices to go down
when the size increases.

23
00:01:00.505 --> 00:01:04.295
Big houses seem like they
should usually cost more.

24
00:01:04.295 --> 00:01:07.930
Then you may choose a
cubic function where we

25
00:01:07.930 --> 00:01:12.015
now have not only x
squared, but x cubed.

26
00:01:12.015 --> 00:01:16.090
Maybe this model produces
this curve here,

27
00:01:16.090 --> 00:01:17.710
which is a somewhat
better fit to

28
00:01:17.710 --> 00:01:19.270
the data because the size

29
00:01:19.270 --> 00:01:23.120
does eventually come back
up as the size increases.

30
00:01:23.120 --> 00:01:26.340
These are both examples
of polynomial regression,

31
00:01:26.340 --> 00:01:29.510
because you took your
optional feature x,

32
00:01:29.510 --> 00:01:31.070
and raised it to the power of

33
00:01:31.070 --> 00:01:34.375
two or three or any other power.

34
00:01:34.375 --> 00:01:36.785
In the case of the
cubic function,

35
00:01:36.785 --> 00:01:38.615
the first feature is the size,

36
00:01:38.615 --> 00:01:40.865
the second feature
is the size squared,

37
00:01:40.865 --> 00:01:43.840
and the third feature
is the size cubed.

38
00:01:43.840 --> 00:01:46.940
I just want to point
out one more thing,

39
00:01:46.940 --> 00:01:49.280
which is that if you
create features that are

40
00:01:49.280 --> 00:01:51.545
these powers like the square

41
00:01:51.545 --> 00:01:53.405
of the original
features like this,

42
00:01:53.405 --> 00:01:57.310
then feature scaling becomes
increasingly important.

43
00:01:57.310 --> 00:02:00.425
If the size of the
house ranges from say,

44
00:02:00.425 --> 00:02:02.555
1-1,000 square feet,

45
00:02:02.555 --> 00:02:04.430
then the second feature,

46
00:02:04.430 --> 00:02:06.020
which is a size squared,

47
00:02:06.020 --> 00:02:08.665
will range from
one to a million,

48
00:02:08.665 --> 00:02:10.440
and the third feature,

49
00:02:10.440 --> 00:02:11.790
which is size cubed,

50
00:02:11.790 --> 00:02:14.965
ranges from one to a billion.

51
00:02:14.965 --> 00:02:18.440
These two features, x
squared and x cubed,

52
00:02:18.440 --> 00:02:20.450
take on very different ranges of

53
00:02:20.450 --> 00:02:23.260
values compared to the
original feature x.

54
00:02:23.260 --> 00:02:25.165
If you're using
gradient descent,

55
00:02:25.165 --> 00:02:28.100
it's important to apply
feature scaling to get

56
00:02:28.100 --> 00:02:31.990
your features into
comparable ranges of values.

57
00:02:31.990 --> 00:02:35.390
Finally, just one last
example of how you

58
00:02:35.390 --> 00:02:38.810
really have a wide range of
choices of features to use.

59
00:02:38.810 --> 00:02:40.970
Another reasonable
alternative to

60
00:02:40.970 --> 00:02:42.470
taking the size squared and

61
00:02:42.470 --> 00:02:46.630
size cubed is to say use
the square root of x.

62
00:02:46.630 --> 00:02:50.415
Your model may look
like w_1 times

63
00:02:50.415 --> 00:02:55.395
x plus w_2 times the
square root of x plus b.

64
00:02:55.395 --> 00:02:57.875
The square root function
looks like this,

65
00:02:57.875 --> 00:03:01.850
and it becomes a bit less
steep as x increases,

66
00:03:01.850 --> 00:03:04.130
but it doesn't ever
completely flatten out,

67
00:03:04.130 --> 00:03:07.315
and it certainly never
ever comes back down.

68
00:03:07.315 --> 00:03:09.980
This would be another
choice of features that

69
00:03:09.980 --> 00:03:12.790
might work well for
this data-set as well.

70
00:03:12.790 --> 00:03:14.840
You may ask yourself,

71
00:03:14.840 --> 00:03:17.525
how do I decide what
features to use?

72
00:03:17.525 --> 00:03:20.690
Later in the second course
in this specialization,

73
00:03:20.690 --> 00:03:23.630
you see how you can choose
different features and

74
00:03:23.630 --> 00:03:25.205
different models that include

75
00:03:25.205 --> 00:03:26.875
or don't include these features,

76
00:03:26.875 --> 00:03:30.050
and you have a process
for measuring how well

77
00:03:30.050 --> 00:03:32.390
these different models
perform to help you

78
00:03:32.390 --> 00:03:35.320
decide which features to
include or not include.

79
00:03:35.320 --> 00:03:37.955
For now, I just want
you to be aware

80
00:03:37.955 --> 00:03:40.685
that you have a choice in
what features you use.

81
00:03:40.685 --> 00:03:44.075
By using feature engineering
and polynomial functions,

82
00:03:44.075 --> 00:03:45.440
you can potentially get

83
00:03:45.440 --> 00:03:48.175
a much better model
for your data.

84
00:03:48.175 --> 00:03:51.440
In the optional lab that
follows this video,

85
00:03:51.440 --> 00:03:53.780
you will see some
code that implements

86
00:03:53.780 --> 00:03:56.735
polynomial regression
using features like x,

87
00:03:56.735 --> 00:03:58.460
x squared, and x cubed.

88
00:03:58.460 --> 00:04:02.710
Please take a look and run the
code and see how it works.

89
00:04:02.710 --> 00:04:05.265
There's also another
optional lab

90
00:04:05.265 --> 00:04:07.160
after that one that shows how to

91
00:04:07.160 --> 00:04:09.380
use a popular open
source toolkit

92
00:04:09.380 --> 00:04:12.150
that implements
linear regression.

93
00:04:12.470 --> 00:04:14.660
Scikit-learn is

94
00:04:14.660 --> 00:04:18.065
a very widely used open source
machine learning library

95
00:04:18.065 --> 00:04:20.075
that is used by
many practitioners

96
00:04:20.075 --> 00:04:21.500
in many of the top AI,

97
00:04:21.500 --> 00:04:24.890
internet, machine learning
companies in the world.

98
00:04:26.060 --> 00:04:28.815
If either now or in the future

99
00:04:28.815 --> 00:04:30.830
you're using machine
learning in your job,

100
00:04:30.830 --> 00:04:32.930
there's a very good
chance you'll be using

101
00:04:32.930 --> 00:04:36.630
tools like Scikit-learn
to train your models.

102
00:04:37.100 --> 00:04:40.310
Working through that
optional lab will give you

103
00:04:40.310 --> 00:04:43.400
a chance to not only better
understand linear regression,

104
00:04:43.400 --> 00:04:46.340
but also see how
this can be done in

105
00:04:46.340 --> 00:04:47.990
just a few lines of code using

106
00:04:47.990 --> 00:04:50.560
a library like Scikit-learn.

107
00:04:50.560 --> 00:04:53.180
For you to have a
solid understanding

108
00:04:53.180 --> 00:04:54.245
of these algorithms,

109
00:04:54.245 --> 00:04:55.985
and be able to apply them,

110
00:04:55.985 --> 00:04:57.680
I do think is important that you

111
00:04:57.680 --> 00:04:59.840
know how to implement
linear regression

112
00:04:59.840 --> 00:05:01.820
yourself and not just call

113
00:05:01.820 --> 00:05:04.610
some scikit-learn function
that is a black-box.

114
00:05:04.610 --> 00:05:06.560
But scikit-learn also has

115
00:05:06.560 --> 00:05:08.180
an important role in a way

116
00:05:08.180 --> 00:05:11.190
machine learning is
done in practice today.

117
00:05:11.210 --> 00:05:14.625
We're just about at
the end of this week.

118
00:05:14.625 --> 00:05:18.000
Congratulations on finishing
all of this week's videos.

119
00:05:18.000 --> 00:05:19.940
Please do take a
look at the practice

120
00:05:19.940 --> 00:05:22.400
quizzes and also
the practice lab,

121
00:05:22.400 --> 00:05:24.530
which I hope will
let you try out and

122
00:05:24.530 --> 00:05:27.165
practice ideas that
we've discussed.

123
00:05:27.165 --> 00:05:29.305
In this week's practice lab,

124
00:05:29.305 --> 00:05:31.285
you implement linear regression.

125
00:05:31.285 --> 00:05:33.200
I hope you have a
lot of fun getting

126
00:05:33.200 --> 00:05:35.935
this learning algorithm
to work for yourself.

127
00:05:35.935 --> 00:05:37.860
Best of luck with that.

128
00:05:37.860 --> 00:05:41.750
I also look forward to seeing
you in next week's videos,

129
00:05:41.750 --> 00:05:43.400
where we'll go
beyond regression,

130
00:05:43.400 --> 00:05:44.890
that is predicting numbers,

131
00:05:44.890 --> 00:05:47.600
to talk about our first
classification algorithm,

132
00:05:47.600 --> 00:05:49.580
which can predict categories.

133
00:05:49.580 --> 00:05:51.990
I'll see you next week.