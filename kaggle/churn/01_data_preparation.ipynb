{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load the Data\n",
    "\n",
    "In 2018, KKBox, a popular music streaming service based in Taiwan, released a dataset consisting of a little over two years of (anonymized) customer transaction and activity data with the goal of challenging the Data & AI community to predict which customers would churn in a future period.\n",
    "\n",
    "NOTE Due to the terms and conditions by which these data are made available, anyone interested in recreating this work will need to agree with the terms and conditions before making up this dataset and create a similar folder structure as described below in their environment. You can save the data permanently under a pre-defined mount point named /mnt/kkbox:\n",
    "\n",
    "We have automated this downloading step for you and used a /tmp/kkbox_churn storage path throughout this accelerator.\n",
    "\n",
    "Read into dataframes, these files form the following data model:\n",
    "\n",
    "Each service subscriber is uniquely identified by a value in the msno field of the members table. Data in the transactions and user logs tables provide a record of subscription management and streaming activities, respectively. Not every member has a complete set of data in this schema. In addition, the transaction and streaming logs are quite verbose with multiple records being recorded for a subscriber on a given date. On dates where there is no activity, no entries are found for a subscriber in these tables.\n",
    "\n",
    "In order to protect data privacy, many values in these tables have been ordinal-encoded, limiting their interpretability. In addition, timestamp information has been truncated to a daily level, making the sequencing of records on a given date dependent on business logic addressed in later steps in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit\n",
    "import shutil\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has been added for scenarios where you might\n",
    "# wish to alter some of the churn label prediction\n",
    "# logic but do not wish to rerun the whole notebook\n",
    "skip_reload = False\n",
    "\n",
    "# please use a personalized database name here if you wish to avoid interfering with other users who might be running this accelerator in the same workspace\n",
    "database_name = 'kkbox_churn'\n",
    "data_dir = f\"{os.getenv('HOME')}/databricks/kkbox_churn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()  # Properly stop Spark\n",
    "del spark     # Delete the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ChurnCluster\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.executor.memory\", \"56g\") \\\n",
    "    .config(\"spark.driver.memory\", \"48g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "os.environ[\"SPARK_APP_NAME\"] = spark.conf.get(\"spark.app.name\")\n",
    "os.environ[\"SPARK_MASTER\"] = spark.conf.get(\"spark.master\")\n",
    "\n",
    "print(\"Spark Version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip_reload:\n",
    "  # create database to house SQL tables\n",
    "  _ = spark.sql(f'CREATE DATABASE IF NOT EXISTS {database_name}')\n",
    "  _ = spark.sql(f'USE {database_name}')\n",
    "else:\n",
    "  # delete the old database if needed\n",
    "  _ = spark.sql(f'DROP DATABASE IF EXISTS {database_name} CASCADE')\n",
    "  _ = spark.sql(f'CREATE DATABASE {database_name}')\n",
    "  _ = spark.sql(f'USE {database_name}')\n",
    "\n",
    "  # drop any old delta lake files that might have been created\n",
    "  folder_path = f'{data_dir}/silver/members'\n",
    "  if os.path.exists(folder_path):\n",
    "      shutil.rmtree(folder_path)\n",
    "    \n",
    "  # members dataset schema\n",
    "  member_schema = StructType([\n",
    "    StructField('msno', StringType()),\n",
    "    StructField('city', IntegerType()),\n",
    "    StructField('bd', IntegerType()),\n",
    "    StructField('gender', StringType()),\n",
    "    StructField('registered_via', IntegerType()),\n",
    "    StructField('registration_init_time', DateType())\n",
    "    ])\n",
    "\n",
    "  # read data from csv\n",
    "  members = (\n",
    "    spark\n",
    "      .read\n",
    "      .csv(\n",
    "        f'{data_dir}/members/members_v3.csv',\n",
    "        schema=member_schema,\n",
    "        header=True,\n",
    "        dateFormat='yyyyMMdd'\n",
    "        )\n",
    "      )\n",
    "\n",
    "  # persist in delta lake format\n",
    "  (\n",
    "    members\n",
    "      .write\n",
    "      .format('delta')\n",
    "      .mode('overwrite')\n",
    "      .save(f'{data_dir}/silver/members')\n",
    "    )\n",
    "\n",
    "    # create table object to make delta lake queryable\n",
    "  _ = spark.sql('''\n",
    "      CREATE TABLE members \n",
    "      USING DELTA \n",
    "      LOCATION '/home/dinindu/databricks/kkbox_churn/silver/members'\n",
    "      ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(members.show())\n",
    "result = spark.sql(\"SELECT * FROM kkbox_churn.members LIMIT 10\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_reload:\n",
    "\n",
    "  # drop any old delta lake files that might have been created\n",
    "  folder_path = f'{data_dir}/silver/transactions'\n",
    "  if os.path.exists(folder_path):\n",
    "      shutil.rmtree(folder_path)\n",
    "\n",
    "  # transaction dataset schema\n",
    "  transaction_schema = StructType([\n",
    "    StructField('msno', StringType()),\n",
    "    StructField('payment_method_id', IntegerType()),\n",
    "    StructField('payment_plan_days', IntegerType()),\n",
    "    StructField('plan_list_price', IntegerType()),\n",
    "    StructField('actual_amount_paid', IntegerType()),\n",
    "    StructField('is_auto_renew', IntegerType()),\n",
    "    StructField('transaction_date', DateType()),\n",
    "    StructField('membership_expire_date', DateType()),\n",
    "    StructField('is_cancel', IntegerType())  \n",
    "    ])\n",
    "\n",
    "  # read data from csv\n",
    "  transactions = (\n",
    "    spark\n",
    "      .read\n",
    "      .csv(\n",
    "        f'{data_dir}/transactions/transactions.csv',\n",
    "        schema=transaction_schema,\n",
    "        header=True,\n",
    "        dateFormat='yyyyMMdd'\n",
    "        )\n",
    "      )\n",
    "\n",
    "  # persist in delta lake format\n",
    "  ( transactions\n",
    "      .write\n",
    "      .format('delta')\n",
    "      .partitionBy('transaction_date')\n",
    "      .mode('overwrite')\n",
    "      .save(f'{data_dir}/silver/transactions')\n",
    "    )\n",
    "\n",
    "    # create table object to make delta lake queryable\n",
    "  _ = spark.sql('''\n",
    "      CREATE TABLE transactions\n",
    "      USING DELTA \n",
    "      LOCATION '/home/dinindu/databricks/kkbox_churn/silver/transactions'\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(transactions.show())\n",
    "result = spark.sql(\"SELECT * FROM kkbox_churn.transactions\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_reload:\n",
    "  # drop any old delta lake files that might have been created\n",
    "  folder_path = f'{data_dir}/silver/user_logs'\n",
    "  if os.path.exists(folder_path):\n",
    "      shutil.rmtree(folder_path)\n",
    "\n",
    "  # transaction dataset schema\n",
    "  user_logs_schema = StructType([ \n",
    "    StructField('msno', StringType()),\n",
    "    StructField('date', DateType()),\n",
    "    StructField('num_25', IntegerType()),\n",
    "    StructField('num_50', IntegerType()),\n",
    "    StructField('num_75', IntegerType()),\n",
    "    StructField('num_985', IntegerType()),\n",
    "    StructField('num_100', IntegerType()),\n",
    "    StructField('num_uniq', IntegerType()),\n",
    "    StructField('total_secs', FloatType())  \n",
    "    ])\n",
    "\n",
    "  # read data from csv\n",
    "  user_logs = (\n",
    "    spark\n",
    "      .read\n",
    "      .csv(\n",
    "        f'{data_dir}/user_logs/user_logs.csv',\n",
    "        schema=user_logs_schema,\n",
    "        header=True,\n",
    "        dateFormat='yyyyMMdd'\n",
    "        )\n",
    "      )\n",
    "\n",
    "  # persist in delta lake format\n",
    "  ( user_logs\n",
    "      .write\n",
    "      .format('delta')\n",
    "      .partitionBy('date')\n",
    "      .mode('overwrite')\n",
    "      .save(f'{data_dir}/silver/user_logs')\n",
    "    )\n",
    "\n",
    "  # create table object to make delta lake queryable\n",
    "  _ = spark.sql('''\n",
    "    CREATE TABLE IF NOT EXISTS user_logs\n",
    "    USING DELTA \n",
    "    LOCATION '/home/dinindu/databricks/kkbox_churn/silver/user_logs'\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Acquire Churn Labels\n",
    "\n",
    "To build our model, we will need to identify which customers have churned within two periods of interest. These periods are February 2017 and March 2017. We will train our model to predict churn in February 2017 and then evaluate our model's ability to predict churn in March 2017, making these our training and testing datasets, respectively.\n",
    "\n",
    "Per instructions provided in the Kaggle competition, a KKBox subscriber is not identified as churned until he or she fails to renew their subscription 30-days following its expiration. Most subscriptions are themselves on a 30-day renewal schedule (though some subscriptions renew on significantly longer cycles). This means that identifying churn involves a sequential walk through the customer data, looking for renewal gaps that would indicate a customer churned on a prior expiration date.\n",
    "\n",
    "While the competition makes available pre-labeled training and testing datasets, train.csv and train_v2.csv, respectively, several past participants have noted that these datasets should be regenerated. A Scala script for doing so is provided by KKBox. Modifying the script for this environment, we might regenerate our training and test datasets as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete training labels if exists before create\n",
    "_ = spark.sql('DROP TABLE IF EXISTS train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sh -e\n",
    "\n",
    "# Create a spark cluster. But not required.\n",
    "# docker network create spark-net\n",
    "\n",
    "# docker run -d --rm --network spark-net --name spark-master \\\n",
    "#     -p 8080:8080 -p 7077:7077 -p 4040:4040 \\\n",
    "#     bitnami/spark spark-class org.apache.spark.deploy.master.Master\n",
    "\n",
    "# docker run -d --rm --network spark-net --name spark-worker \\\n",
    "#     --env SPARK_MODE=worker \\\n",
    "#     --env SPARK_MASTER_URL=spark://spark-master:7077 \\\n",
    "#     bitnami/spark spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -e\n",
    "# Generate training labels\n",
    "\n",
    "kkbox_churn_dir=\"/home/dinindu/databricks/kkbox_churn\"\n",
    "sudo chmod 777 $kkbox_churn_dir\n",
    "sudo rm -rf $kkbox_churn_dir/silver/train\n",
    "\n",
    "docker run --rm --network host  \\\n",
    "    -v \"$kkbox_churn_dir:/opt/spark/work/kkbox_churn\" \\\n",
    "    -v \"$PWD:/opt/bitnami/spark/work\" \\\n",
    "    bitnami/spark:3.4.1 spark-shell --master local[*] \\\n",
    "    --executor-memory 48G \\\n",
    "    --driver-memory 16G \\\n",
    "    --packages io.delta:delta-core_2.12:2.4.0 \\\n",
    "    --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n",
    "    --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\\n",
    "    -i /opt/bitnami/spark/work/scripts/generate_training_labels.scala\n",
    "\n",
    "sudo chown -R dinindu:dinindu $kkbox_churn_dir/silver/train\n",
    "sudo chmod -R 777 $kkbox_churn_dir/silver/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the training labels\n",
    "_ = spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS train\n",
    "USING DELTA\n",
    "LOCATION '/home/dinindu/databricks/kkbox_churn/silver/train/'\n",
    "''')\n",
    "\n",
    "_ = spark.sql('SELECT * FROM train').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete testing labels if exists before create\n",
    "_ = spark.sql('DROP TABLE IF EXISTS test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -e\n",
    "# Generate testing labels\n",
    "\n",
    "kkbox_churn_dir=\"/home/dinindu/databricks/kkbox_churn\"\n",
    "sudo chmod 777 $kkbox_churn_dir\n",
    "sudo rm -rf $kkbox_churn_dir/silver/test\n",
    "\n",
    "docker run --rm --network host  \\\n",
    "    -v \"$kkbox_churn_dir:/opt/spark/work/kkbox_churn\" \\\n",
    "    -v \"$PWD:/opt/bitnami/spark/work\" \\\n",
    "    bitnami/spark:3.4.1 spark-shell --master local[*] \\\n",
    "    --executor-memory 48G \\\n",
    "    --driver-memory 16G \\\n",
    "    --packages io.delta:delta-core_2.12:2.4.0 \\\n",
    "    --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n",
    "    --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\\n",
    "    -i /opt/bitnami/spark/work/scripts/generate_testing_labels.scala\n",
    "\n",
    "sudo chown -R dinindu:dinindu $kkbox_churn_dir/silver/test\n",
    "sudo chmod -R 777 $kkbox_churn_dir/silver/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the testing labels\n",
    "_ = spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS test\n",
    "USING DELTA\n",
    "LOCATION '/home/dinindu/databricks/kkbox_churn/silver/test/'\n",
    "              ''')\n",
    "\n",
    "_ = spark.sql('SELECT * FROM test').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Cleanse & Enhance Transaction Logs\n",
    "\n",
    "In the churn script provided by KKBox (and used in the last step), time between transaction events is used in order to determine churn status. In situations where multiple transactions are recorded on a given date, complex logic is used to determine which transaction represents the final state of the account on that date. This logic states that when we have multiple transactions for a given subscriber on a given date, we should:\n",
    "\n",
    "1. Concatenate the plan_list_price, payment_plan_days, and payment_method_id values and consider the \"bigger\" of these values as preceding the others\n",
    "2. If the concatenated value (defined in the last step) is the same across records for this date, cancellations, i.e. records where is_cancel=1, should follow other transactions\n",
    "3. If there are multiple cancellations in this sequence, the record with the earliest expiration date is the last record for this transaction date\n",
    "4. If there are no cancellations but multiple non-cancellations in this sequence, the non-cancellation record with the latest expiration date is the last record on the transaction date\n",
    "\n",
    "Rewriting this logic in SQL allows us to generate a cleansed version of the transaction log with the final record for each date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -e\n",
    "sudo rm -rf $PWD/spark-warehouse/transactions_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the SQL file\n",
    "with open(\"scripts/01_transactions_clean.sql\", \"r\") as sql_file:\n",
    "    sql_script = sql_file.read()\n",
    "\n",
    "# Execute the SQL script\n",
    "for statement in sql_script.split(\";\"):\n",
    "    statement = statement.strip()\n",
    "    if statement:\n",
    "        spark.sql(statement)\n",
    "\n",
    "spark.sql(\"SELECT * FROM transactions_clean LIMIT 10\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this cleansed transaction data, we can now more easily identify the start and end of subscriptions using the 30-day gap logic found in the Scala code. It's important to note that over the 2+ year period represented by the dataset, many subscribers will churn and many of those that do churn will re-subscribe. With this in mind, we will generate a subscription ID to identify the different subscriptions, each of which will have a non-overlapping starting and ending date for a given subscriber:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -e\n",
    "sudo rm -rf $PWD/spark-warehouse/subscription_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the SQL file\n",
    "with open(\"scripts/01_subscription_windows.sql\", \"r\") as sql_file:\n",
    "    sql_script = sql_file.read()\n",
    "\n",
    "# Execute the SQL script\n",
    "for statement in sql_script.split(\";\"):\n",
    "    statement = statement.strip()\n",
    "    if statement:\n",
    "        spark.sql(statement)\n",
    "\n",
    "spark.sql(\"SELECT * FROM subscription_windows LIMIT 10\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To verify we have our subscription windows aligned with the script used to identify customers at-risk for churn in February and March 2017, let's perform a quick test. The script identifies an at-risk subscription as one where the last transaction recorded in the historical period, i.e. the time period leading up to the start of the month of interest, has an expiration date falling between the 30-day window leading up to the start of the period of interest and the end of that period. For example, if we were to identify at-risk customers for February 2017, we should look for those customers with active subscriptions set to expire within the 30-days before February 1, 2017 and February 28, 2017. This shifted window allows time for the 30-day grace period to expire within the period of interest.\n",
    "\n",
    "NOTE Better logic would limit our assessment to those subscriptions with an expiration date between 30-days prior to the start of the period AND 30-days prior to the end of the period. (Such logic would exclude subscriptions expiring within the period of interest but which do not exit the 30-day grace period until after the period is over.) When we use this logic, we find numerous subscriptions that the provided script identifies as at-risk but which we would not. We will align our logic with that of the competition for this exercise.\n",
    "\n",
    "With this logic in mind, let's see if all our labeled at-risk customers adhere to this logic:\n",
    "\n",
    "NOTE The next two cells should return NO RESULTS if our logic is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Any Subscriptions in Training Dataset Not Believed to Be At-Risk\n",
    "spark.sql('''\n",
    "SELECT\n",
    "  x.msno\n",
    "FROM train x\n",
    "LEFT OUTER JOIN (\n",
    "  SELECT DISTINCT -- subscriptions that had risk in Feb 2017\n",
    "    a.msno\n",
    "  FROM subscription_windows a\n",
    "  INNER JOIN transactions_clean b\n",
    "    ON a.msno=b.msno AND b.transaction_date BETWEEN a.subscription_start AND a.subscription_end\n",
    "  WHERE \n",
    "        a.subscription_start < '2017-02-01' AND\n",
    "        (b.membership_expire_date BETWEEN DATE_ADD('2017-02-01',-30) AND '2017-02-28')\n",
    "  ) y\n",
    "  ON x.msno=y.msno\n",
    "WHERE y.msno IS NULL\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Any Subscriptions in Testing Dataset Not Believed to Be At-Risk\n",
    "spark.sql('''\n",
    "SELECT\n",
    "  x.msno\n",
    "FROM test x\n",
    "LEFT OUTER JOIN (\n",
    "  SELECT DISTINCT -- subscriptions that had risk in Feb 2017\n",
    "    a.msno\n",
    "  FROM subscription_windows a\n",
    "  INNER JOIN transactions_clean b\n",
    "    ON a.msno=b.msno AND b.transaction_date BETWEEN a.subscription_start AND a.subscription_end\n",
    "  WHERE \n",
    "        a.subscription_start < '2017-03-01' AND\n",
    "        (b.membership_expire_date BETWEEN DATE_ADD('2017-03-01',-30) AND '2017-03-31')\n",
    "  ) y\n",
    "  ON x.msno=y.msno\n",
    "WHERE y.msno IS NULL\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we do not fail to identify the same at-risk subscriptions as the provided script, if we were to alter the code above we would find a few subscriptions that we do identify as at-risk but which the Scala script does not. While it might be useful to examine why this is, so long as there are no members that the Scala script identifies as at risk that we do not, we should should be able to use this dataset to derive features for subscriptions in our testing and training datasets.\n",
    "\n",
    "Leveraging subscription duration information derived in the last few cells, we can now enhance our transaction log to detect account-level changes. This information will form the basis for transaction-feature generation in the next notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -e\n",
    "sudo rm -rf $PWD/spark-warehouse/transactions_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the SQL file\n",
    "with open(\"scripts/01_transactions_enhanced.sql\", \"r\") as sql_file:\n",
    "    sql_script = sql_file.read()\n",
    "\n",
    "# Execute the SQL script\n",
    "for statement in sql_script.split(\";\"):\n",
    "    statement = statement.strip()\n",
    "    if statement:\n",
    "        spark.sql(statement)\n",
    "\n",
    "spark.sql(\"SELECT * FROM transactions_enhanced LIMIT 10\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Generate Dates Table\n",
    "\n",
    "Finally, it is very likely we will want to derive features from both the transaction log and the user activity data where we examine days without activity. To make this analysis easier, it may be helpful to generate a table containing one record for each date from the beginning date to the end date in our dataset. We know that these data span January 1, 2015 through March 31, 2017. With that in mind, we can generate such a table as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate days in range\n",
    "start_date = date(2015, 1, 1)\n",
    "end_date = date(2017, 3, 31)\n",
    "days = end_date - start_date\n",
    "\n",
    "# generate temp view of dates in range\n",
    "( spark\n",
    "    .range(0, days.days)  \n",
    "    .withColumn('start_date', lit(start_date.strftime('%Y-%m-%d')))  # first date in activity dataset\n",
    "    .selectExpr('date_add(start_date, CAST(id as int)) as date')\n",
    "    .write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \n",
    "    .saveAsTable('dates')\n",
    "  )\n",
    "\n",
    "# display SQL table content\n",
    "display(spark.table('dates').orderBy('date'))\n",
    "\n",
    "spark.sql(\"SELECT * FROM dates\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
